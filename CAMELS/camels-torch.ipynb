{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8ee0f-74c0-435c-85c3-9c65a1ec1f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from multifield_combined import MultifieldDataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de6580-3bb7-4938-93f1-eb9fb8a815c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "#cudnn.benchmark = True      #May train faster but cost more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662270bb-817b-4b13-81cb-3f45ef79a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of data\n",
    "DDIR = '/home/masterdesky/data/CAMELS/2D_maps/data/' # Data directory\n",
    "TARGET = 'SIMBA'#'IllustrisTNG'                      # Target simulations\n",
    "\n",
    "FILE = os.listdir(DDIR)\n",
    "FILE = sorted([\n",
    "    f for f in FILE if ('.txt' not in f) & ('_CV_' not in f) \\\n",
    "                     & ('Nbody' not in f) & (TARGET in f)\n",
    "])\n",
    "\n",
    "# Data parameters\n",
    "fdata      = [FILE[0]]\n",
    "fmaps      = [f\"{'_'.join(f.split('_')[:2])}_{TARGET}.npy\" for f in fdata]\n",
    "fmaps_norm = [None] #if you want to normalize the maps according to the properties of some data set, put that data set here (This is mostly used when training on IllustrisTNG and testing on SIMBA, or vicerversa)\n",
    "fparams    = os.path.join(DDIR, f\"params_{TARGET}.txt\")\n",
    "splits     = 6   #number of maps per simulation (1 - 15)\n",
    "seed       = 1   #random seed to split maps among training, validation and testing\n",
    "\n",
    "# Training parameters\n",
    "channels   = 1                   # Number of fields to consider\n",
    "params     = [0, 1, 2, 3, 4, 5]  # Omega_m, sigma_8, A_SN1, A_AGN1, A_SN2, A_AGN2\n",
    "g          = params              # Mean of the posterior\n",
    "h          = [6+i for i in g]    # Variance of the posterior\n",
    "memmap     = False               # Keep rotations and flippings in memory\n",
    "\n",
    "# Optimizer parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "lr         = 1e-3\n",
    "wd         = 0.0005  #value of weight decay\n",
    "dr         = 0.2     #dropout value for fully connected layers\n",
    "hidden     = 5       #this determines the number of layers in the CNNs; > 1\n",
    "epochs     = 100     #number of epochs to train the network\n",
    "\n",
    "# output files names\n",
    "SUFFIX = '_'.join([f.split('_')[1] for f in fmaps])\n",
    "floss  = f\"loss_{TARGET}_{SUFFIX}.txt\"   #file with the training and validation losses for each epoch\n",
    "fmodel = f\"weights_{TARGET}_{SUFFIX}.pt\" #file containing the weights of the best-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daaaf57-b093-4f27-a42e-4fdab5d6128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_dataset(fdata, splits, *, verbose=True):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fdata : str\n",
    "        Name of the file containing the selected dataset.\n",
    "    splits : int\n",
    "        Number of maps to select for every simulation. Should be between\n",
    "        1 and 15. (At least 1 map should be chosen out of the total 15.)\n",
    "    verbose : bool\n",
    "        Controls verbosity.\n",
    "    '''\n",
    "    assert 1 <= splits <= 15, \"Value of `splits` should be between 1 and 15!\"\n",
    "    \n",
    "    maps = np.load(os.path.join(DDIR, fdata))\n",
    "    if verbose:\n",
    "        print(f\"Shape of the maps: {maps.shape}\")\n",
    "        \n",
    "    # define the array that will contain the indexes of the maps\n",
    "    indexes = np.arange(15000)%15 < splits\n",
    "    if verbose:\n",
    "        print(f\"Selected {np.sum(indexes):,} maps out of 15,000\")\n",
    "\n",
    "    # Save these maps to a new file\n",
    "    np.save(f\"{'_'.join(fdata.split('_')[:2])}_{TARGET}.npy\", maps[indexes])\n",
    "    del maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8a158-ad36-42e4-bb2e-7d66c7f17ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fin in fdata:\n",
    "    prepare_raw_dataset(fdata=fin, splits=6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73970882-deef-48f5-9cfb-e4c4ae86059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This routine returns the data loader need to train the network\n",
    "def create_dataset_multifield(mode, fmaps, fmaps_norm, fparams,\n",
    "                              splits, batch_size, *, memmap=True,\n",
    "                              shuffle=True, seed=None, verbose=False):\n",
    "\n",
    "    # whether rotations and flippings are kept in memory\n",
    "    data_set = MultifieldDataset(\n",
    "        mode, fmaps, fmaps_norm, fparams, splits,\n",
    "        norm_params=True, memmap=memmap, seed=seed, verbose=True\n",
    "    )\n",
    "    data_loader = DataLoader(\n",
    "        dataset=data_set, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7ac86-77ee-4d4d-8ecd-dc3e2b0ebd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training set\n",
    "print('\\nPreparing training set')\n",
    "train_loader = create_dataset_multifield('train', fmaps, fmaps_norm, fparams,\n",
    "                                         splits, batch_size, memmap=memmap,\n",
    "                                         shuffle=True, seed=seed, verbose=True)\n",
    "\n",
    "# get validation set\n",
    "print('\\nPreparing validation set')\n",
    "valid_loader = create_dataset_multifield('valid', fmaps, fmaps_norm, fparams,\n",
    "                                         splits, batch_size, memmap=True,\n",
    "                                         shuffle=True, seed=seed, verbose=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7180fc-e19a-489b-a578-560c37ffa75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_so3(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_channels, n_filters,\n",
    "                 kernel_size, padding, padding_mode, stride,\n",
    "                 dropout_rate=0.2):\n",
    "        super(model_so3, self).__init__()\n",
    "\n",
    "        # Possible activation functions\n",
    "        self.ReLU      = nn.ReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        self.tanh      = nn.Tanh()\n",
    "\n",
    "        # Input parameter dependent modules\n",
    "        self.dropout   = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        # input: 1x256x256 ---------------> output: 2*hiddenx128x128\n",
    "        self.C01 = nn.Conv2d(n_channels, 2*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C02 = nn.Conv2d(2*n_filters, 2*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C03 = nn.Conv2d(2*n_filters, 2*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B01 = nn.BatchNorm2d(2*hidden)\n",
    "        self.B02 = nn.BatchNorm2d(2*hidden)\n",
    "        self.B03 = nn.BatchNorm2d(2*hidden)\n",
    "        \n",
    "        # input: 2*hiddenx128x128 ----------> output: 4*hiddenx64x64\n",
    "        self.C11 = nn.Conv2d(2*n_filters, 4*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C12 = nn.Conv2d(4*n_filters, 4*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C13 = nn.Conv2d(4*n_filters, 4*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B11 = nn.BatchNorm2d(4*hidden)\n",
    "        self.B12 = nn.BatchNorm2d(4*hidden)\n",
    "        self.B13 = nn.BatchNorm2d(4*hidden)\n",
    "        \n",
    "        # input: 4*hiddenx64x64 --------> output: 8*hiddenx32x32\n",
    "        self.C21 = nn.Conv2d(4*n_filters, 8*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B21 = nn.BatchNorm2d(8*n_filters)\n",
    "        self.C22 = nn.Conv2d(8*n_filters, 8*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B22 = nn.BatchNorm2d(8*n_filters)\n",
    "        self.C23 = nn.Conv2d(8*n_filters, 8*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B23 = nn.BatchNorm2d(8*n_filters)\n",
    "        \n",
    "        # input: 8*hiddenx32x32 ----------> output: 16*hiddenx16x16\n",
    "        self.C31 = nn.Conv2d(8*n_filters, 16*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B31 = nn.BatchNorm2d(16*n_filters)\n",
    "        self.C32 = nn.Conv2d(16*n_filters, 16*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B32 = nn.BatchNorm2d(16*n_filters)\n",
    "        self.C33 = nn.Conv2d(16*n_filters, 16*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B33 = nn.BatchNorm2d(16*n_filters)\n",
    "        \n",
    "        # input: 16*hiddenx16x16 ----------> output: 32*hiddenx8x8\n",
    "        self.C41 = nn.Conv2d(16*n_filters, 32*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B41 = nn.BatchNorm2d(32*n_filters)\n",
    "        self.C42 = nn.Conv2d(32*n_filters, 32*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B42 = nn.BatchNorm2d(32*n_filters)\n",
    "        self.C43 = nn.Conv2d(32*n_filters, 32*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)        \n",
    "        self.B43 = nn.BatchNorm2d(32*n_filters)\n",
    "        \n",
    "        # input: 32*hiddenx8x8 ----------> output:64*hiddenx4x4\n",
    "        self.C51 = nn.Conv2d(32*n_filters, 64*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B51 = nn.BatchNorm2d(64*n_filters)\n",
    "        self.C52 = nn.Conv2d(64*n_filters, 64*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B52 = nn.BatchNorm2d(64*n_filters)\n",
    "        self.C53 = nn.Conv2d(64*n_filters, 64*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B53 = nn.BatchNorm2d(64*n_filters)\n",
    "\n",
    "        # input: 64*hiddenx4x4 ----------> output: 128*hiddenx1x1\n",
    "        self.C61 = nn.Conv2d(64*n_filters, 128*n_filters,\n",
    "                             kernel_size=4,\n",
    "                             stride=4,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B61 = nn.BatchNorm2d(128*n_filters)\n",
    "\n",
    "        self.P0  = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.FC1  = nn.Linear(128*n_filters, 64*n_filters)  \n",
    "        self.FC2  = nn.Linear(64*n_filters,  12)\n",
    "\n",
    "        # Set conventional values for batch normalization modules\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        x = self.LeakyReLU(self.B01(self.C01(image)))\n",
    "        x = self.LeakyReLU(self.B02(self.C02(x)))\n",
    "        x = self.LeakyReLU(self.B03(self.C03(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B11(self.C11(x)))\n",
    "        x = self.LeakyReLU(self.B12(self.C12(x)))\n",
    "        x = self.LeakyReLU(self.B13(self.C13(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B21(self.C21(x)))\n",
    "        x = self.LeakyReLU(self.B22(self.C22(x)))\n",
    "        x = self.LeakyReLU(self.B23(self.C23(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B31(self.C31(x)))\n",
    "        x = self.LeakyReLU(self.B32(self.C32(x)))\n",
    "        x = self.LeakyReLU(self.B33(self.C33(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B41(self.C41(x)))\n",
    "        x = self.LeakyReLU(self.B42(self.C42(x)))\n",
    "        x = self.LeakyReLU(self.B43(self.C43(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B51(self.C51(x)))\n",
    "        x = self.LeakyReLU(self.B52(self.C52(x)))\n",
    "        x = self.LeakyReLU(self.B53(self.C53(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B61(self.C61(x)))\n",
    "\n",
    "        x = x.view(image.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dropout(self.LeakyReLU(self.FC1(x)))\n",
    "        x = self.FC2(x)\n",
    "\n",
    "        # enforce the errors to be positive\n",
    "        y = torch.clone(x)\n",
    "        y[:,6:12] = torch.square(x[:,6:12])\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51ec79-51e7-4b93-bb0d-6b65c1a97ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_so3(\n",
    "    n_channels=1,\n",
    "    n_filters=5,\n",
    "    kernel_size=3,\n",
    "    padding=1,\n",
    "    padding_mode='circular',\n",
    "    stride=1,\n",
    "    dropout_rate=0.2\n",
    ")\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067a508-9060-4d82-ba82-8ab8aeea322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15366d-4e1a-45d2-ab69-2dca97469c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that architecture is defined above, use it\n",
    "network_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"total number of parameters in the model = {network_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bcb8f7-95bc-42bc-bd86-a105bc247d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=wd,\n",
    "    betas=(beta1, beta2)\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    'min',\n",
    "    factor=0.3,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db703ea8-745f-400e-88c8-28358d690cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing initial validation loss')\n",
    "model.eval()\n",
    "valid_loss1 = torch.zeros(len(g)).to(device)\n",
    "valid_loss2 = torch.zeros(len(g)).to(device)\n",
    "min_valid_loss, points = 0.0, 0\n",
    "for x, y in valid_loader:\n",
    "    with torch.no_grad():\n",
    "        bs   = x.shape[0]                #batch size\n",
    "        x    = x.to(device=device)       #maps\n",
    "        y    = y.to(device=device)[:,g]  #parameters\n",
    "        p    = model(x)                  #NN output\n",
    "        y_NN = p[:,g]                    #posterior mean\n",
    "        e_NN = p[:,h]                    #posterior std\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "        loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "        valid_loss1 += loss1*bs\n",
    "        valid_loss2 += loss2*bs\n",
    "        points += bs\n",
    "min_valid_loss = torch.log(valid_loss1/points) + torch.log(valid_loss2/points)\n",
    "min_valid_loss = torch.mean(min_valid_loss).item()\n",
    "print('Initial valid loss = %.3e'%min_valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322cead-7c9b-4a10-ace5-13ea5c3a5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# do a loop over all epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # do training\n",
    "    train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    train_loss, points = 0.0, 0\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        bs   = x.shape[0]         #batch size\n",
    "        x    = x.to(device)       #maps\n",
    "        y    = y.to(device)[:,g]  #parameters\n",
    "        p    = model(x)           #NN output\n",
    "        y_NN = p[:,g]             #posterior mean\n",
    "        e_NN = p[:,h]             #posterior std\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "        loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "        train_loss1 += loss1*bs\n",
    "        train_loss2 += loss2*bs\n",
    "        points      += bs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if points>18000:  break\n",
    "    train_loss = torch.log(train_loss1/points) + torch.log(train_loss2/points)\n",
    "    train_loss = torch.mean(train_loss).item()\n",
    "\n",
    "    # do validation: cosmo alone & all params\n",
    "    valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    valid_loss, points = 0.0, 0\n",
    "    model.eval()\n",
    "    for x, y in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            bs    = x.shape[0]         #batch size\n",
    "            x     = x.to(device)       #maps\n",
    "            y     = y.to(device)[:,g]  #parameters\n",
    "            p     = model(x)           #NN output\n",
    "            y_NN  = p[:,g]             #posterior mean\n",
    "            e_NN  = p[:,h]             #posterior std\n",
    "            loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "            valid_loss1 += loss1*bs\n",
    "            valid_loss2 += loss2*bs\n",
    "            points     += bs\n",
    "    valid_loss = torch.log(valid_loss1/points) + torch.log(valid_loss2/points)\n",
    "    valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    # verbose\n",
    "    print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "\n",
    "    # save model if it is better\n",
    "    if valid_loss<min_valid_loss:\n",
    "        torch.save(model.state_dict(), fmodel)\n",
    "        min_valid_loss = valid_loss\n",
    "        print('(C) ', end='')\n",
    "    print('')\n",
    "\n",
    "    # save losses to file\n",
    "    f = open(floss, 'a')\n",
    "    f.write('%d %.5e %.5e\\n'%(epoch, train_loss, valid_loss))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b01c5-0620-4572-8be3-f6bc97f38f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights in case they exists\n",
    "if os.path.exists(fmodel):  \n",
    "    model.load_state_dict(torch.load(fmodel, map_location=torch.device(device)))\n",
    "    print('Weights loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728cb93-4835-4145-a1a4-130653e0d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "test_loader  = create_dataset_multifield('test', fmaps, fmaps_norm, fparams,\n",
    "                                         splits, batch_size, memmap=False,\n",
    "                                         shuffle=True, seed=seed, verbose=True)  \n",
    "\n",
    "# get the number of maps in the test set\n",
    "num_maps = 0\n",
    "for x, y in test_loader:\n",
    "      num_maps += x.shape[0]\n",
    "print('\\nNumber of maps in the test set: %d'%num_maps)\n",
    "\n",
    "# define the arrays containing the value of the parameters\n",
    "params_true = np.zeros((num_maps,6), dtype=np.float32)\n",
    "params_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "errors_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "\n",
    "# get test loss\n",
    "test_loss1, test_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "test_loss, points = 0.0, 0\n",
    "model.eval()\n",
    "for x, y in test_loader:\n",
    "    with torch.no_grad():\n",
    "        bs    = x.shape[0]    #batch size\n",
    "        x     = x.to(device)  #send data to device\n",
    "        y     = y.to(device)  #send data to device\n",
    "        p     = model(x)      #prediction for mean and variance\n",
    "        y_NN  = p[:,:6]       #prediction for mean\n",
    "        e_NN  = p[:,6:]       #prediction for error\n",
    "        loss1 = torch.mean((y_NN[:,g] - y[:,g])**2,                     axis=0)\n",
    "        loss2 = torch.mean(((y_NN[:,g] - y[:,g])**2 - e_NN[:,g]**2)**2, axis=0)\n",
    "        test_loss1 += loss1*bs\n",
    "        test_loss2 += loss2*bs\n",
    "\n",
    "        # save results to their corresponding arrays\n",
    "        params_true[points:points+x.shape[0]] = y.cpu().numpy() \n",
    "        params_NN[points:points+x.shape[0]]   = y_NN.cpu().numpy()\n",
    "        errors_NN[points:points+x.shape[0]]   = e_NN.cpu().numpy()\n",
    "        points    += x.shape[0]\n",
    "test_loss = torch.log(test_loss1/points) + torch.log(test_loss2/points)\n",
    "test_loss = torch.mean(test_loss).item()\n",
    "print('Test loss = %.3e\\n'%test_loss)\n",
    "\n",
    "Norm_error = np.sqrt(np.mean((params_true - params_NN)**2, axis=0))\n",
    "print('Normalized Error Omega_m = %.3f'%Norm_error[0])\n",
    "print('Normalized Error sigma_8 = %.3f'%Norm_error[1])\n",
    "print('Normalized Error A_SN1   = %.3f'%Norm_error[2])\n",
    "print('Normalized Error A_AGN1  = %.3f'%Norm_error[3])\n",
    "print('Normalized Error A_SN2   = %.3f'%Norm_error[4])\n",
    "print('Normalized Error A_AGN2  = %.3f\\n'%Norm_error[5])\n",
    "\n",
    "# de-normalize\n",
    "minimum = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "maximum = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "params_true = params_true*(maximum - minimum) + minimum\n",
    "params_NN   = params_NN*(maximum - minimum) + minimum\n",
    "errors_NN   = errors_NN*(maximum - minimum)\n",
    "\n",
    "error = np.sqrt(np.mean((params_true - params_NN)**2, axis=0))\n",
    "print('Error Omega_m = %.3f'%error[0])\n",
    "print('Error sigma_8 = %.3f'%error[1])\n",
    "print('Error A_SN1   = %.3f'%error[2])\n",
    "print('Error A_AGN1  = %.3f'%error[3])\n",
    "print('Error A_SN2   = %.3f'%error[4])\n",
    "print('Error A_AGN2  = %.3f\\n'%error[5])\n",
    "\n",
    "mean_error = np.absolute(np.mean(errors_NN, axis=0))\n",
    "print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "print('Bayesian error A_SN1   = %.3f'%mean_error[2])\n",
    "print('Bayesian error A_AGN1  = %.3f'%mean_error[3])\n",
    "print('Bayesian error A_SN2   = %.3f'%mean_error[4])\n",
    "print('Bayesian error A_AGN2  = %.3f\\n'%mean_error[5])\n",
    "\n",
    "rel_error = np.sqrt(np.mean((params_true - params_NN)**2/params_true**2, axis=0))\n",
    "print('Relative error Omega_m = %.3f'%rel_error[0])\n",
    "print('Relative error sigma_8 = %.3f'%rel_error[1])\n",
    "print('Relative error A_SN1   = %.3f'%rel_error[2])\n",
    "print('Relative error A_AGN1  = %.3f'%rel_error[3])\n",
    "print('Relative error A_SN2   = %.3f'%rel_error[4])\n",
    "print('Relative error A_AGN2  = %.3f\\n'%rel_error[5])\n",
    "\n",
    "# save results to file\n",
    "#dataset = np.zeros((num_maps,18), dtype=np.float32)\n",
    "#dataset[:,:6]   = params_true\n",
    "#dataset[:,6:12] = params_NN\n",
    "#dataset[:,12:]  = errors_NN\n",
    "#np.savetxt(fresults,  dataset)\n",
    "#np.savetxt(fresults1, Norm_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1b890-72d6-4522-8633-f609dd5c7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first map of every simulation in the test set\n",
    "indexes = np.arange(50)*splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf94c4-97f7-43ab-a7d8-0ab3d1098d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.1, 0.45, r'$\\Omega_{\\rm m}$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,0], params_NN[indexes,0], errors_NN[indexes,0], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.1,0.5], [0.1,0.5], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9d33c-1923-44be-8714-3fc76268bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.6, 0.95, r'$\\sigma_8$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,1], params_NN[indexes,1], errors_NN[indexes,1], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.6,1.0], [0.6,1.0], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf844ff-9ad0-467c-b80d-fea160312bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.25, 4.0, r'$A_{\\rm SN1}$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,2], params_NN[indexes,2], errors_NN[indexes,2], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.25,4.0], [0.25,4.0], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52e31e-d290-4703-8048-2fde48e89291",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.5, 2.0, r'$A_{\\rm SN2}$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,4], params_NN[indexes,4], errors_NN[indexes,4], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.5,2.0], [0.5,2.0], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7b712-e2a4-49d1-9292-51207c36560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.25, 4.0, r'$A_{\\rm AGN1}$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,3], params_NN[indexes,3], errors_NN[indexes,3], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.25,4.0], [0.25,4.0], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b39edb-da1e-4833-87a7-00f3fef144b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.5, 2.0, r'$A_{\\rm AGN2}$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,5], params_NN[indexes,5], errors_NN[indexes,5], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.5,2.0], [0.5,2.0], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738f058-23ea-4ed9-9c23-928cb872882b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28529d06-100e-4386-9332-fe2fb47ce8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8086a2a-a331-4d26-acae-b003f8829878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f2ef7-8a69-4a1e-973d-1ce34821da10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdeaed0-9762-460a-8d66-456cc2cff410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692f453-d8bd-4719-a4a7-a874f24858eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd3524-2f26-4402-8747-fd7b02d36568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df7e88-1f1c-4714-90ac-5f30a8857a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92098ea2-b1ef-4a7c-8b32-2d73f1248b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0f8b5-5b20-4a58-a20f-f73f79ff6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sims = 600\n",
    "offset, size_sims = int(0.95*n_sims), int(0.05*n_sims)\n",
    "size_maps = size_sims*splits\n",
    "splits = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017af1bd-c516-4816-8662-dc64e92c2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the simulations (indeces from 0 to 999 in case of CAMELS)\n",
    "np.random.seed(None)\n",
    "sim_numbers = np.arange(n_sims) #shuffle sims not maps\n",
    "np.random.shuffle(sim_numbers)\n",
    "sim_numbers = sim_numbers[offset:offset+size_sims] #select indexes of mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe1919-c746-4603-9ef1-c94b2206f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.array(\n",
    "    np.repeat(sim_numbers * splits, splits) \\\n",
    "  + np.tile(range(splits), len(sim_numbers)), dtype=np.int32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3a66f-edd7-4a8b-a07d-e0aebce5f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.zeros(size_maps, dtype=np.int32)\n",
    "count = 0\n",
    "for i in sim_numbers:\n",
    "    for j in range(splits):\n",
    "        indexes[count] = i*splits + j\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95b7fc-ab0c-4199-b66b-86529da63b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
