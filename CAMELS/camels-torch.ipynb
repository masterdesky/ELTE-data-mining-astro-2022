{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8ee0f-74c0-435c-85c3-9c65a1ec1f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from multifield_combined import MultifieldDataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de6580-3bb7-4938-93f1-eb9fb8a815c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "cudnn.benchmark = True      # May train faster but cost more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662270bb-817b-4b13-81cb-3f45ef79a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of data\n",
    "PDIR = \"/mnt/local/scratch_sata_stripe/scratch/masterdesky/data/CAMELS\"\n",
    "DDIR = os.path.join(PDIR, \"2D_maps/data/\")  # Data directory\n",
    "TARGET = 'IllustrisTNG'#'SIMBA'             # Target simulations\n",
    "\n",
    "FILE = os.listdir(DDIR)\n",
    "FILE = sorted([\n",
    "    f for f in FILE if ('.txt' not in f) & ('_CV_' not in f) \\\n",
    "                     & ('Nbody' not in f) & (TARGET in f)\n",
    "])\n",
    "\n",
    "# IO parameters and paths\n",
    "PREFIX = os.path.join(PDIR, 'output')\n",
    "FDATA      = [FILE[12]]\n",
    "FMAPS      = [\n",
    "  os.path.join(PREFIX, f\"{'_'.join(f.split('_')[:2])}_{TARGET}.npy\") \\\n",
    "  for f in FDATA\n",
    "]\n",
    "FMAPS_NORM = [None] #if you want to normalize the maps according to the properties of some data set, put that data set here (This is mostly used when training on IllustrisTNG and testing on SIMBA, or vicerversa)\n",
    "FPARAMS    = os.path.join(DDIR, f\"params_{TARGET}.txt\")\n",
    "SUFFIX = '_'.join([f.split('_')[1] for f in FDATA])\n",
    "FLOSS  = os.path.join(PREFIX, f\"loss_{TARGET}_{SUFFIX}.txt\")   # validation losses\n",
    "FMODEL = os.path.join(PREFIX, f\"weights_{TARGET}_{SUFFIX}.pt\") # weights of best model\n",
    "\n",
    "# Data parameters\n",
    "splits     = 6   #number of maps per simulation (1 - 15)\n",
    "seed       = 1   #random seed to split maps among training, validation and testing\n",
    "\n",
    "# Training parameters\n",
    "channels   = len(FDATA)          # Number of fields to consider\n",
    "params     = [0, 1, 2, 3, 4, 5]  # Omega_m, sigma_8, A_SN1, A_AGN1, A_SN2, A_AGN2\n",
    "g          = params              # Mean of the posterior\n",
    "h          = [6+i for i in g]    # Variance of the posterior\n",
    "memmap     = False               # Keep rotations and flippings in memory\n",
    "\n",
    "# Optimizer parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "lr         = 1e-3\n",
    "wd         = 0.0005  #value of weight decay\n",
    "dr         = 0.2     #dropout value for fully connected layers\n",
    "hidden     = 5       #this determines the number of layers in the CNNs; > 1\n",
    "epochs     = 100     #number of epochs to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daaaf57-b093-4f27-a42e-4fdab5d6128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_dataset(FDATA, splits, *, verbose=True):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    FDATA : str\n",
    "        Name of the file containing the selected dataset.\n",
    "    splits : int\n",
    "        Number of maps to select for every simulation. Should be between\n",
    "        1 and 15. (At least 1 map should be chosen out of the total 15.)\n",
    "    verbose : bool\n",
    "        Controls verbosity.\n",
    "    '''\n",
    "    assert 1 <= splits <= 15, \"Value of `splits` should be between 1 and 15!\"\n",
    "    \n",
    "    maps = np.load(os.path.join(DDIR, FDATA))\n",
    "    if verbose:\n",
    "        print(f\"Shape of the maps: {maps.shape}\")\n",
    "        \n",
    "    # define the array that will contain the indexes of the maps\n",
    "    indexes = np.arange(15000)%15 < splits\n",
    "    if verbose:\n",
    "        print(f\"Selected {np.sum(indexes):,} maps out of 15,000\")\n",
    "\n",
    "    # Save these maps to a new file\n",
    "    ffile = f\"{'_'.join(FDATA.split('_')[:2])}_{TARGET}.npy\"\n",
    "    fsave = os.path.join(PREFIX, ffile)\n",
    "    np.save(fsave, maps[indexes])\n",
    "    del maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8a158-ad36-42e4-bb2e-7d66c7f17ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fin in FDATA:\n",
    "#    prepare_raw_dataset(FDATA=fin, splits=splits, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73970882-deef-48f5-9cfb-e4c4ae86059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This routine returns the data loader need to train the network\n",
    "def create_dataset_multifield(mode, FMAPS, FMAPS_NORM, FPARAMS,\n",
    "                              splits, batch_size, *, memmap=True,\n",
    "                              shuffle=True, seed=None, verbose=False):\n",
    "\n",
    "    # whether rotations and flippings are kept in memory\n",
    "    data_set = MultifieldDataset(\n",
    "        mode, FMAPS, FMAPS_NORM, FPARAMS, splits,\n",
    "        norm_params=True, memmap=memmap, seed=seed, verbose=True\n",
    "    )\n",
    "    data_loader = DataLoader(\n",
    "        dataset=data_set, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7ac86-77ee-4d4d-8ecd-dc3e2b0ebd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training set\n",
    "print('\\nPreparing training set')\n",
    "train_loader = create_dataset_multifield('train', FMAPS, FMAPS_NORM, FPARAMS,\n",
    "                                         splits, batch_size, memmap=memmap,\n",
    "                                         shuffle=True, seed=seed, verbose=True)\n",
    "\n",
    "# get validation set\n",
    "print('\\nPreparing validation set')\n",
    "valid_loader = create_dataset_multifield('valid', FMAPS, FMAPS_NORM, FPARAMS,\n",
    "                                         splits, batch_size, memmap=True,\n",
    "                                         shuffle=True, seed=seed, verbose=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7180fc-e19a-489b-a578-560c37ffa75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_so3(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_channels, n_filters,\n",
    "                 kernel_size, padding, padding_mode, stride,\n",
    "                 dropout_rate=0.2):\n",
    "        super(model_so3, self).__init__()\n",
    "\n",
    "        # Possible activation functions\n",
    "        self.ReLU      = nn.ReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        self.tanh      = nn.Tanh()\n",
    "\n",
    "        # Input parameter dependent modules\n",
    "        self.dropout   = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        # input: 1x256x256 ---------------> output: 2*hiddenx128x128\n",
    "        self.C01 = nn.Conv2d(n_channels, 2*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C02 = nn.Conv2d(2*n_filters, 2*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C03 = nn.Conv2d(2*n_filters, 2*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B01 = nn.BatchNorm2d(2*hidden)\n",
    "        self.B02 = nn.BatchNorm2d(2*hidden)\n",
    "        self.B03 = nn.BatchNorm2d(2*hidden)\n",
    "        \n",
    "        # input: 2*hiddenx128x128 ----------> output: 4*hiddenx64x64\n",
    "        self.C11 = nn.Conv2d(2*n_filters, 4*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C12 = nn.Conv2d(4*n_filters, 4*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C13 = nn.Conv2d(4*n_filters, 4*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B11 = nn.BatchNorm2d(4*hidden)\n",
    "        self.B12 = nn.BatchNorm2d(4*hidden)\n",
    "        self.B13 = nn.BatchNorm2d(4*hidden)\n",
    "        \n",
    "        # input: 4*hiddenx64x64 --------> output: 8*hiddenx32x32\n",
    "        self.C21 = nn.Conv2d(4*n_filters, 8*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B21 = nn.BatchNorm2d(8*n_filters)\n",
    "        self.C22 = nn.Conv2d(8*n_filters, 8*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B22 = nn.BatchNorm2d(8*n_filters)\n",
    "        self.C23 = nn.Conv2d(8*n_filters, 8*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B23 = nn.BatchNorm2d(8*n_filters)\n",
    "        \n",
    "        # input: 8*hiddenx32x32 ----------> output: 16*hiddenx16x16\n",
    "        self.C31 = nn.Conv2d(8*n_filters, 16*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B31 = nn.BatchNorm2d(16*n_filters)\n",
    "        self.C32 = nn.Conv2d(16*n_filters, 16*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B32 = nn.BatchNorm2d(16*n_filters)\n",
    "        self.C33 = nn.Conv2d(16*n_filters, 16*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B33 = nn.BatchNorm2d(16*n_filters)\n",
    "        \n",
    "        # input: 16*hiddenx16x16 ----------> output: 32*hiddenx8x8\n",
    "        self.C41 = nn.Conv2d(16*n_filters, 32*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B41 = nn.BatchNorm2d(32*n_filters)\n",
    "        self.C42 = nn.Conv2d(32*n_filters, 32*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B42 = nn.BatchNorm2d(32*n_filters)\n",
    "        self.C43 = nn.Conv2d(32*n_filters, 32*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)        \n",
    "        self.B43 = nn.BatchNorm2d(32*n_filters)\n",
    "        \n",
    "        # input: 32*hiddenx8x8 ----------> output:64*hiddenx4x4\n",
    "        self.C51 = nn.Conv2d(32*n_filters, 64*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B51 = nn.BatchNorm2d(64*n_filters)\n",
    "        self.C52 = nn.Conv2d(64*n_filters, 64*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B52 = nn.BatchNorm2d(64*n_filters)\n",
    "        self.C53 = nn.Conv2d(64*n_filters, 64*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B53 = nn.BatchNorm2d(64*n_filters)\n",
    "\n",
    "        # input: 64*hiddenx4x4 ----------> output: 128*hiddenx1x1\n",
    "        self.C61 = nn.Conv2d(64*n_filters, 128*n_filters,\n",
    "                             kernel_size=4,\n",
    "                             stride=4,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B61 = nn.BatchNorm2d(128*n_filters)\n",
    "\n",
    "        self.P0  = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.FC1  = nn.Linear(128*n_filters, 64*n_filters)  \n",
    "        self.FC2  = nn.Linear(64*n_filters,  12)\n",
    "\n",
    "        # Set conventional values for batch normalization modules\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        x = self.LeakyReLU(self.B01(self.C01(image)))\n",
    "        x = self.LeakyReLU(self.B02(self.C02(x)))\n",
    "        x = self.LeakyReLU(self.B03(self.C03(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B11(self.C11(x)))\n",
    "        x = self.LeakyReLU(self.B12(self.C12(x)))\n",
    "        x = self.LeakyReLU(self.B13(self.C13(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B21(self.C21(x)))\n",
    "        x = self.LeakyReLU(self.B22(self.C22(x)))\n",
    "        x = self.LeakyReLU(self.B23(self.C23(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B31(self.C31(x)))\n",
    "        x = self.LeakyReLU(self.B32(self.C32(x)))\n",
    "        x = self.LeakyReLU(self.B33(self.C33(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B41(self.C41(x)))\n",
    "        x = self.LeakyReLU(self.B42(self.C42(x)))\n",
    "        x = self.LeakyReLU(self.B43(self.C43(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B51(self.C51(x)))\n",
    "        x = self.LeakyReLU(self.B52(self.C52(x)))\n",
    "        x = self.LeakyReLU(self.B53(self.C53(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B61(self.C61(x)))\n",
    "\n",
    "        x = x.view(image.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dropout(self.LeakyReLU(self.FC1(x)))\n",
    "        x = self.FC2(x)\n",
    "\n",
    "        # enforce the errors to be positive\n",
    "        y = torch.clone(x)\n",
    "        y[:,6:12] = torch.square(x[:,6:12])\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51ec79-51e7-4b93-bb0d-6b65c1a97ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_so3(\n",
    "    n_channels=1,\n",
    "    n_filters=5,\n",
    "    kernel_size=3,\n",
    "    padding=1,\n",
    "    padding_mode='circular',\n",
    "    stride=1,\n",
    "    dropout_rate=0.2\n",
    ")\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067a508-9060-4d82-ba82-8ab8aeea322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15366d-4e1a-45d2-ab69-2dca97469c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that architecture is defined above, use it\n",
    "network_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"total number of parameters in the model = {network_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bcb8f7-95bc-42bc-bd86-a105bc247d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=wd,\n",
    "    betas=(beta1, beta2)\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    'min',\n",
    "    factor=0.3,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db703ea8-745f-400e-88c8-28358d690cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing initial validation loss')\n",
    "model.eval()\n",
    "valid_loss1 = torch.zeros(len(g)).to(device)\n",
    "valid_loss2 = torch.zeros(len(g)).to(device)\n",
    "min_valid_loss, points = 0.0, 0\n",
    "for x, y in valid_loader:\n",
    "    with torch.no_grad():\n",
    "        bs   = x.shape[0]                #batch size\n",
    "        x    = x.to(device=device)       #maps\n",
    "        y    = y.to(device=device)[:,g]  #parameters\n",
    "        p    = model(x)                  #NN output\n",
    "        y_NN = p[:,g]                    #posterior mean\n",
    "        e_NN = p[:,h]                    #posterior std\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "        loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "        valid_loss1 += loss1*bs\n",
    "        valid_loss2 += loss2*bs\n",
    "        points += bs\n",
    "min_valid_loss = torch.log(valid_loss1/points) + torch.log(valid_loss2/points)\n",
    "min_valid_loss = torch.mean(min_valid_loss).item()\n",
    "print('Initial valid loss = %.3e'%min_valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322cead-7c9b-4a10-ace5-13ea5c3a5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# do a loop over all epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # do training\n",
    "    train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    train_loss, points = 0.0, 0\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        bs   = x.shape[0]         #batch size\n",
    "        x    = x.to(device)       #maps\n",
    "        y    = y.to(device)[:,g]  #parameters\n",
    "        p    = model(x)           #NN output\n",
    "        y_NN = p[:,g]             #posterior mean\n",
    "        e_NN = p[:,h]             #posterior std\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "        loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "        train_loss1 += loss1*bs\n",
    "        train_loss2 += loss2*bs\n",
    "        points      += bs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if points>18000:  break\n",
    "    train_loss = torch.log(train_loss1/points) + torch.log(train_loss2/points)\n",
    "    train_loss = torch.mean(train_loss).item()\n",
    "\n",
    "    # do validation: cosmo alone & all params\n",
    "    valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    valid_loss, points = 0.0, 0\n",
    "    model.eval()\n",
    "    for x, y in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            bs    = x.shape[0]         #batch size\n",
    "            x     = x.to(device)       #maps\n",
    "            y     = y.to(device)[:,g]  #parameters\n",
    "            p     = model(x)           #NN output\n",
    "            y_NN  = p[:,g]             #posterior mean\n",
    "            e_NN  = p[:,h]             #posterior std\n",
    "            loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "            valid_loss1 += loss1*bs\n",
    "            valid_loss2 += loss2*bs\n",
    "            points     += bs\n",
    "    valid_loss = torch.log(valid_loss1/points) + torch.log(valid_loss2/points)\n",
    "    valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    # verbose\n",
    "    print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "\n",
    "    # save model if it is better\n",
    "    if valid_loss<min_valid_loss:\n",
    "        torch.save(model.state_dict(), FMODEL)\n",
    "        min_valid_loss = valid_loss\n",
    "        print('(C) ', end='')\n",
    "    print('')\n",
    "\n",
    "    # save losses to file\n",
    "    f = open(FLOSS, 'a')\n",
    "    f.write('%d %.5e %.5e\\n'%(epoch, train_loss, valid_loss))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b01c5-0620-4572-8be3-f6bc97f38f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights in case they exists\n",
    "if os.path.exists(FMODEL):\n",
    "    model.load_state_dict(torch.load(FMODEL, map_location=torch.device(device)))\n",
    "    print('Weights loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728cb93-4835-4145-a1a4-130653e0d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "test_loader  = create_dataset_multifield('test', FMAPS, FMAPS_NORM, FPARAMS,\n",
    "                                         splits, batch_size, memmap=False,\n",
    "                                         shuffle=True, seed=seed, verbose=True)  \n",
    "\n",
    "# get the number of maps in the test set\n",
    "num_maps = 0\n",
    "for x, y in test_loader:\n",
    "      num_maps += x.shape[0]\n",
    "print('\\nNumber of maps in the test set: %d'%num_maps)\n",
    "\n",
    "# define the arrays containing the value of the parameters\n",
    "params_true = np.zeros((num_maps,6), dtype=np.float32)\n",
    "params_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "errors_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "\n",
    "# get test loss\n",
    "test_loss1, test_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "test_loss, points = 0.0, 0\n",
    "model.eval()\n",
    "for x, y in test_loader:\n",
    "    with torch.no_grad():\n",
    "        bs    = x.shape[0]    #batch size\n",
    "        x     = x.to(device)  #send data to device\n",
    "        y     = y.to(device)  #send data to device\n",
    "        p     = model(x)      #prediction for mean and variance\n",
    "        y_NN  = p[:,:6]       #prediction for mean\n",
    "        e_NN  = p[:,6:]       #prediction for error\n",
    "        loss1 = torch.mean((y_NN[:,g] - y[:,g])**2,                     axis=0)\n",
    "        loss2 = torch.mean(((y_NN[:,g] - y[:,g])**2 - e_NN[:,g]**2)**2, axis=0)\n",
    "        test_loss1 += loss1*bs\n",
    "        test_loss2 += loss2*bs\n",
    "\n",
    "        # save results to their corresponding arrays\n",
    "        params_true[points:points+x.shape[0]] = y.cpu().numpy() \n",
    "        params_NN[points:points+x.shape[0]]   = y_NN.cpu().numpy()\n",
    "        errors_NN[points:points+x.shape[0]]   = e_NN.cpu().numpy()\n",
    "        points    += x.shape[0]\n",
    "test_loss = torch.log(test_loss1/points) + torch.log(test_loss2/points)\n",
    "test_loss = torch.mean(test_loss).item()\n",
    "print('Test loss = %.3e\\n'%test_loss)\n",
    "\n",
    "Norm_error = np.sqrt(np.mean((params_true - params_NN)**2, axis=0))\n",
    "print('Normalized Error Omega_m = %.3f'%Norm_error[0])\n",
    "print('Normalized Error sigma_8 = %.3f'%Norm_error[1])\n",
    "print('Normalized Error A_SN1   = %.3f'%Norm_error[2])\n",
    "print('Normalized Error A_AGN1  = %.3f'%Norm_error[3])\n",
    "print('Normalized Error A_SN2   = %.3f'%Norm_error[4])\n",
    "print('Normalized Error A_AGN2  = %.3f\\n'%Norm_error[5])\n",
    "\n",
    "# de-normalize\n",
    "minimum = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "maximum = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "params_true = params_true*(maximum - minimum) + minimum\n",
    "params_NN   = params_NN*(maximum - minimum) + minimum\n",
    "errors_NN   = errors_NN*(maximum - minimum)\n",
    "\n",
    "error = np.sqrt(np.mean((params_true - params_NN)**2, axis=0))\n",
    "print('Error Omega_m = %.3f'%error[0])\n",
    "print('Error sigma_8 = %.3f'%error[1])\n",
    "print('Error A_SN1   = %.3f'%error[2])\n",
    "print('Error A_AGN1  = %.3f'%error[3])\n",
    "print('Error A_SN2   = %.3f'%error[4])\n",
    "print('Error A_AGN2  = %.3f\\n'%error[5])\n",
    "\n",
    "mean_error = np.absolute(np.mean(errors_NN, axis=0))\n",
    "print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "print('Bayesian error A_SN1   = %.3f'%mean_error[2])\n",
    "print('Bayesian error A_AGN1  = %.3f'%mean_error[3])\n",
    "print('Bayesian error A_SN2   = %.3f'%mean_error[4])\n",
    "print('Bayesian error A_AGN2  = %.3f\\n'%mean_error[5])\n",
    "\n",
    "rel_error = np.sqrt(np.mean((params_true - params_NN)**2/params_true**2, axis=0))\n",
    "print('Relative error Omega_m = %.3f'%rel_error[0])\n",
    "print('Relative error sigma_8 = %.3f'%rel_error[1])\n",
    "print('Relative error A_SN1   = %.3f'%rel_error[2])\n",
    "print('Relative error A_AGN1  = %.3f'%rel_error[3])\n",
    "print('Relative error A_SN2   = %.3f'%rel_error[4])\n",
    "print('Relative error A_AGN2  = %.3f\\n'%rel_error[5])\n",
    "\n",
    "# save results to file\n",
    "#dataset = np.zeros((num_maps,18), dtype=np.float32)\n",
    "#dataset[:,:6]   = params_true\n",
    "#dataset[:,6:12] = params_NN\n",
    "#dataset[:,12:]  = errors_NN\n",
    "#np.savetxt(fresults,  dataset)\n",
    "#np.savetxt(fresults1, Norm_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1b890-72d6-4522-8633-f609dd5c7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first map of every simulation in the test set\n",
    "indexes = np.arange(params_true.shape[0]//splits)*splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329a929-1569-44eb-bd35-23bc1dfd691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rc('text', usetex=False)\n",
    "nr, nc = 2, 3\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(6*nc, 6*nr))\n",
    "fig.subplots_adjust(hspace=0.15)\n",
    "\n",
    "# 0: Omega_m, 1: sigma_8, 2: A_SN1, 3: A_AGN1, 4: A_SN2, 5: A_AGN2 \n",
    "order = [0, 1, 3, 2, 4, 5]\n",
    "labels = [\n",
    "  \"$\\Omega_{m}$\", \"$\\sigma_{8}$\",\n",
    "  \"$A_{SN1}$\", \"$A_{AGN1}$\", \"$A_{SN2}$\", \"$A_{AGN2}$\"\n",
    "]\n",
    "limits = [\n",
    "  [0.1, 0.5], [0.6, 1.0],\n",
    "  [0.25, 4.0], [0.25, 4.0], [0.5, 2.0], [0.5, 2.0]\n",
    "]\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "  i = order[idx]\n",
    "  ax.set_xlabel(f\"Groundtruth\", fontsize=14, fontweight='bold')\n",
    "  ax.set_ylabel(f\"Prediction\", fontsize=14, fontweight='bold')\n",
    "  ax.text(0.1, 0.85, labels[i],fontsize=18, transform=ax.transAxes)\n",
    "  \n",
    "  ax.errorbar(x=params_true[indexes, i],\n",
    "              y=params_NN[indexes, i],\n",
    "              yerr=errors_NN[indexes, i],\n",
    "              linestyle='None', fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "  \n",
    "  ax.plot(limits[i], limits[i],\n",
    "          color='black')\n",
    "  \n",
    "  ODIR = '/home/masterdesky/projects/camels/out/'\n",
    "  if not os.path.exists(ODIR):\n",
    "    os.makedirs(ODIR)\n",
    "  plt.savefig(os.path.join(ODIR, f\"predictions_{TARGET}_{SUFFIX}.png\"),\n",
    "              bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738f058-23ea-4ed9-9c23-928cb872882b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
