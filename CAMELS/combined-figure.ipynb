{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e5fa5-c84e-4835-bd86-3defb1e035c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from multifield_combined import MultifieldDataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215cc7fa-0eea-4a80-95d4-dee064b7bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "cudnn.benchmark = True      # May train faster but cost more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95829d-0432-414a-8ed5-62393517e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This routine returns the data loader need to train the network\n",
    "def create_dataset_multifield(mode, FMAPS, FMAPS_NORM, FPARAMS,\n",
    "                              splits, batch_size, *, memmap=True,\n",
    "                              shuffle=True, seed=None, verbose=False):\n",
    "\n",
    "    # whether rotations and flippings are kept in memory\n",
    "    data_set = MultifieldDataset(\n",
    "        mode, FMAPS, FMAPS_NORM, FPARAMS, splits,\n",
    "        norm_params=True, memmap=memmap, seed=seed, verbose=True\n",
    "    )\n",
    "    data_loader = DataLoader(\n",
    "        dataset=data_set, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1472d-f884-43b4-b1c1-6586d46e560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_so3(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_channels, n_filters,\n",
    "                 kernel_size, padding, padding_mode, stride,\n",
    "                 dropout_rate=0.2):\n",
    "        super(model_so3, self).__init__()\n",
    "\n",
    "        # Possible activation functions\n",
    "        self.ReLU      = nn.ReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        self.tanh      = nn.Tanh()\n",
    "\n",
    "        # Input parameter dependent modules\n",
    "        self.dropout   = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        # input: 1x256x256 ---------------> output: 2*hiddenx128x128\n",
    "        self.C01 = nn.Conv2d(n_channels, 2*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C02 = nn.Conv2d(2*n_filters, 2*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C03 = nn.Conv2d(2*n_filters, 2*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B01 = nn.BatchNorm2d(2*n_filters)\n",
    "        self.B02 = nn.BatchNorm2d(2*n_filters)\n",
    "        self.B03 = nn.BatchNorm2d(2*n_filters)\n",
    "        \n",
    "        # input: 2*hiddenx128x128 ----------> output: 4*hiddenx64x64\n",
    "        self.C11 = nn.Conv2d(2*n_filters, 4*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C12 = nn.Conv2d(4*n_filters, 4*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.C13 = nn.Conv2d(4*n_filters, 4*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B11 = nn.BatchNorm2d(4*n_filters)\n",
    "        self.B12 = nn.BatchNorm2d(4*n_filters)\n",
    "        self.B13 = nn.BatchNorm2d(4*n_filters)\n",
    "        \n",
    "        # input: 4*hiddenx64x64 --------> output: 8*hiddenx32x32\n",
    "        self.C21 = nn.Conv2d(4*n_filters, 8*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B21 = nn.BatchNorm2d(8*n_filters)\n",
    "        self.C22 = nn.Conv2d(8*n_filters, 8*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B22 = nn.BatchNorm2d(8*n_filters)\n",
    "        self.C23 = nn.Conv2d(8*n_filters, 8*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B23 = nn.BatchNorm2d(8*n_filters)\n",
    "        \n",
    "        # input: 8*hiddenx32x32 ----------> output: 16*hiddenx16x16\n",
    "        self.C31 = nn.Conv2d(8*n_filters, 16*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B31 = nn.BatchNorm2d(16*n_filters)\n",
    "        self.C32 = nn.Conv2d(16*n_filters, 16*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B32 = nn.BatchNorm2d(16*n_filters)\n",
    "        self.C33 = nn.Conv2d(16*n_filters, 16*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B33 = nn.BatchNorm2d(16*n_filters)\n",
    "        \n",
    "        # input: 16*hiddenx16x16 ----------> output: 32*hiddenx8x8\n",
    "        self.C41 = nn.Conv2d(16*n_filters, 32*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B41 = nn.BatchNorm2d(32*n_filters)\n",
    "        self.C42 = nn.Conv2d(32*n_filters, 32*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B42 = nn.BatchNorm2d(32*n_filters)\n",
    "        self.C43 = nn.Conv2d(32*n_filters, 32*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)        \n",
    "        self.B43 = nn.BatchNorm2d(32*n_filters)\n",
    "        \n",
    "        # input: 32*hiddenx8x8 ----------> output:64*hiddenx4x4\n",
    "        self.C51 = nn.Conv2d(32*n_filters, 64*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B51 = nn.BatchNorm2d(64*n_filters)\n",
    "        self.C52 = nn.Conv2d(64*n_filters, 64*n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B52 = nn.BatchNorm2d(64*n_filters)\n",
    "        self.C53 = nn.Conv2d(64*n_filters, 64*n_filters,\n",
    "                             kernel_size=2,\n",
    "                             stride=2,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B53 = nn.BatchNorm2d(64*n_filters)\n",
    "\n",
    "        # input: 64*hiddenx4x4 ----------> output: 128*hiddenx1x1\n",
    "        self.C61 = nn.Conv2d(64*n_filters, 128*n_filters,\n",
    "                             kernel_size=4,\n",
    "                             stride=4,\n",
    "                             padding=0, \n",
    "                             padding_mode=padding_mode,\n",
    "                             bias=True)\n",
    "        self.B61 = nn.BatchNorm2d(128*n_filters)\n",
    "\n",
    "        self.P0  = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.FC1  = nn.Linear(128*n_filters, 64*n_filters)  \n",
    "        self.FC2  = nn.Linear(64*n_filters,  12)\n",
    "\n",
    "        # Set conventional values for batch normalization modules\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        x = self.LeakyReLU(self.B01(self.C01(image)))\n",
    "        x = self.LeakyReLU(self.B02(self.C02(x)))\n",
    "        x = self.LeakyReLU(self.B03(self.C03(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B11(self.C11(x)))\n",
    "        x = self.LeakyReLU(self.B12(self.C12(x)))\n",
    "        x = self.LeakyReLU(self.B13(self.C13(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B21(self.C21(x)))\n",
    "        x = self.LeakyReLU(self.B22(self.C22(x)))\n",
    "        x = self.LeakyReLU(self.B23(self.C23(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B31(self.C31(x)))\n",
    "        x = self.LeakyReLU(self.B32(self.C32(x)))\n",
    "        x = self.LeakyReLU(self.B33(self.C33(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B41(self.C41(x)))\n",
    "        x = self.LeakyReLU(self.B42(self.C42(x)))\n",
    "        x = self.LeakyReLU(self.B43(self.C43(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B51(self.C51(x)))\n",
    "        x = self.LeakyReLU(self.B52(self.C52(x)))\n",
    "        x = self.LeakyReLU(self.B53(self.C53(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B61(self.C61(x)))\n",
    "\n",
    "        x = x.view(image.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dropout(self.LeakyReLU(self.FC1(x)))\n",
    "        x = self.FC2(x)\n",
    "\n",
    "        # enforce the errors to be positive\n",
    "        y = torch.clone(x)\n",
    "        y[:,6:12] = torch.square(x[:,6:12])\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c17c8b-9521-4066-ac83-34f8c552eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(fmodel):\n",
    "  \n",
    "  model = model_so3(\n",
    "      n_channels=1,\n",
    "      n_filters=5,\n",
    "      kernel_size=3,\n",
    "      padding=1,\n",
    "      padding_mode='circular',\n",
    "      stride=1,\n",
    "      dropout_rate=0.2\n",
    "  )\n",
    "  model.to(device=device)\n",
    "  model.load_state_dict(torch.load(fmodel, map_location=torch.device(device)))\n",
    "  \n",
    "  return model  \n",
    "  \n",
    "\n",
    "def get_predictions(model,\n",
    "                    fmaps, fmaps_norm, fparams,\n",
    "                    splits, batch_size, seed):\n",
    "  # load test set\n",
    "  test_loader  = create_dataset_multifield('test', fmaps, fmaps_norm, fparams,\n",
    "                                           splits, batch_size, memmap=False,\n",
    "                                           shuffle=True, seed=seed, verbose=True)\n",
    "\n",
    "  # get the number of maps in the test set\n",
    "  num_maps = 0\n",
    "  for x, y in test_loader:\n",
    "        num_maps += x.shape[0]\n",
    "  print('\\nNumber of maps in the test set: %d'%num_maps)\n",
    "\n",
    "  # define the arrays containing the value of the parameters\n",
    "  params_true = np.zeros((num_maps,6), dtype=np.float32)\n",
    "  params_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "  errors_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "\n",
    "  # get test loss\n",
    "  test_loss1, test_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "  test_loss, points = 0.0, 0\n",
    "  model.eval()\n",
    "  for x, y in test_loader:\n",
    "      with torch.no_grad():\n",
    "          bs    = x.shape[0]    #batch size\n",
    "          x     = x.to(device)  #send data to device\n",
    "          y     = y.to(device)  #send data to device\n",
    "          p     = model(x)      #prediction for mean and variance\n",
    "          y_NN  = p[:,:6]       #prediction for mean\n",
    "          e_NN  = p[:,6:]       #prediction for error\n",
    "          loss1 = torch.mean((y_NN[:,g] - y[:,g])**2,                     axis=0)\n",
    "          loss2 = torch.mean(((y_NN[:,g] - y[:,g])**2 - e_NN[:,g]**2)**2, axis=0)\n",
    "          test_loss1 += loss1*bs\n",
    "          test_loss2 += loss2*bs\n",
    "\n",
    "          # save results to their corresponding arrays\n",
    "          params_true[points:points+x.shape[0]] = y.cpu().numpy() \n",
    "          params_NN[points:points+x.shape[0]]   = y_NN.cpu().numpy()\n",
    "          errors_NN[points:points+x.shape[0]]   = e_NN.cpu().numpy()\n",
    "          points    += x.shape[0]\n",
    "  test_loss = torch.log(test_loss1/points) + torch.log(test_loss2/points)\n",
    "  test_loss = torch.mean(test_loss).item()\n",
    "  print('Test loss = %.3e\\n'%test_loss)\n",
    "\n",
    "\n",
    "  # de-normalize\n",
    "  minimum = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "  maximum = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "  params_true = params_true*(maximum - minimum) + minimum\n",
    "  params_NN   = params_NN*(maximum - minimum) + minimum\n",
    "  errors_NN   = errors_NN*(maximum - minimum)\n",
    "  \n",
    "  return params_true, params_NN, errors_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc52c59-5648-484b-8fd5-11d7f703fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_paths(PDIR, DDIR, TARGET='IllustrisTNG'):\n",
    "  FILE = os.listdir(DDIR)\n",
    "  FILE = sorted([\n",
    "      f for f in FILE if ('.txt' not in f) & ('_CV_' not in f) \\\n",
    "                       & ('Nbody' not in f) & (TARGET in f) & ('_B_' not in f)\n",
    "  ])\n",
    "\n",
    "  # IO parameters and paths\n",
    "  PREFIX = os.path.join(PDIR, 'output')\n",
    "  FDATA  = FILE\n",
    "  \n",
    "  FMAPS = [\n",
    "    os.path.join(PREFIX, f\"{'_'.join(f.split('_')[:2])}_{TARGET}.npy\") \\\n",
    "    for f in FDATA\n",
    "  ]\n",
    "  \n",
    "  return PREFIX, FMAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172b6cd-126c-4002-af20-86d2860f980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_paths(DDIR, PREFIX, fdata, TARGET='IllustrisTNG'):\n",
    "  \n",
    "  FPARAMS    = os.path.join(DDIR, f\"params_{TARGET}.txt\")\n",
    "  SUFFIX = fdata.split('/')[-1].split('_')[1]\n",
    "  FLOSS  = os.path.join(PREFIX, f\"loss_{TARGET}_{SUFFIX}.txt\")   # validation losses\n",
    "  FMODEL = os.path.join(PREFIX, f\"weights_{TARGET}_{SUFFIX}.pt\") # weights of best model\n",
    "  \n",
    "  return FPARAMS, FMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee2acc-08c3-49fa-9c34-ece4246041bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of data\n",
    "PDIR = \"/mnt/local/scratch_sata_stripe/scratch/masterdesky/data/CAMELS\"\n",
    "DDIR = os.path.join(PDIR, \"2D_maps/data/\")  # Data directory\n",
    "\n",
    "# Data parameters\n",
    "splits     = 6   #number of maps per simulation (1 - 15)\n",
    "seed       = 1   #random seed to split maps among training, validation and testing\n",
    "\n",
    "# Training parameters\n",
    "params     = [0, 1, 2, 3, 4, 5]  # Omega_m, sigma_8, A_SN1, A_AGN1, A_SN2, A_AGN2\n",
    "g          = params              # Mean of the posterior\n",
    "h          = [6+i for i in g]    # Variance of the posterior\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed3137-40e6-47eb-a01f-4038f8236110",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82554ca8-634a-4a48-9192-361962e0bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd7c0b-341f-4dae-9d0a-8f9e0365f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = 2, 12\n",
    "fig, axes = plt.subplots(nr, nc, figsize=(3*nc, 7*nr),\n",
    "                         sharex=False, sharey=False)\n",
    "fig.subplots_adjust(hspace=0.25, wspace=0)\n",
    "\n",
    "sim_types = ['SIMBA', 'IllustrisTNG']\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "  ax.axvline(x=0, color='black')\n",
    "  ax.set_xlim(-0.5, 0.5)\n",
    "  \n",
    "  ax.tick_params(axis='x', which='major', rotation=90)\n",
    "  if (0 <= i < 11) or (12 <= i < 23):\n",
    "    ax.spines['right'].set_color('white')\n",
    "  if (0 < i <= 11) or (12 < i <= 23):\n",
    "    ax.spines['left'].set_color('white')\n",
    "    ax.set_yticks([])\n",
    "  if (i == 5) or (i == 17):\n",
    "    ax.set_xlabel(f\"$y_{{pred}} - y_{{real}}$ [{sim_types[i]}]\", fontsize=32)\n",
    "\n",
    "for i, TARGET in enumerate(sim_types):\n",
    "  PREFIX, FMAPS = get_data_paths(PDIR, DDIR, TARGET=TARGET)\n",
    "  for j, fdata in enumerate(FMAPS):\n",
    "    ax = axes.reshape(-1)[i*12 + j]\n",
    "    print(f\"Current ax : {i*12 + j}\")\n",
    "    fparams, fmodel = get_inference_paths(DDIR, PREFIX, fdata=fdata, TARGET=TARGET)\n",
    "\n",
    "    model = load_model(fmodel)\n",
    "    fmaps = [fdata]\n",
    "    fmaps_norm = [None]\n",
    "    params_true, params_NN, errors_NN = \\\n",
    "              get_predictions(model,\n",
    "                              fmaps, fmaps_norm, fparams,\n",
    "                              splits, batch_size, seed)\n",
    "    # select the first map of every simulation in the test set\n",
    "    indexes = np.arange(params_true.shape[0]//splits)*splits\n",
    "    # Plot the Omega_m predictions\n",
    "    y_idx = 0\n",
    "    ax.errorbar(y=params_true[indexes, y_idx],\n",
    "                x=params_true[indexes, y_idx] - params_NN[indexes, y_idx],\n",
    "                xerr=errors_NN[indexes, y_idx],\n",
    "                linestyle='None', fmt='o', ms=4, elinewidth=2, capsize=0, c=cm.tab20(j))\n",
    "    ax.text(0.2, 0.8, fmodel.split(f'{TARGET}_')[-1][:-3], rotation='vertical',\n",
    "            fontsize=24, color=cm.tab20(j), transform=ax.transAxes)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556da1ea-632b-4751-b799-8dc09c252e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
